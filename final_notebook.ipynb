{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l1rHOt6-ZYfx"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import argparse\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 5\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "_-DPjLb1vfs6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9vU0jsXxZYfz"
      },
      "outputs": [],
      "source": [
        "class SarcasmDetectionModel(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int, dropout: float):\n",
        "        super(SarcasmDetectionModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True,\n",
        "                            bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
        "        # self.linear = nn.Linear(2 * hidden_dim, 1)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_dim, 36),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(36, 1)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "        text = torch.permute(text, (1, 0, 2))\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
        "            text, text_lengths, enforce_sorted=False)\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
        "            packed_output)\n",
        "        hidden = self.dropout(\n",
        "            torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
        "        return self.linear(hidden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "va4NfX2MZYf0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    # round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()  # convert into float for division\n",
        "    return correct.sum() / len(correct)\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "def load_file(path: str):\n",
        "    \"\"\"\n",
        "    The function load the pickle file and returns him\n",
        "    \"\"\"\n",
        "    with open(path, \"rb\") as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_file_pickle(data: object, path: str):\n",
        "    \"\"\"\n",
        "    The function saves the given data in a pickle file with the given path\n",
        "    \"\"\"\n",
        "    with open(path, \"wb\") as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "\n",
        "def get_max_sentence_length(d: dict) -> int:\n",
        "    max_len = 0\n",
        "    for key, val in d.items():\n",
        "        if len(val) > max_len:\n",
        "            max_len = len(val)\n",
        "    return max_len\n",
        "\n",
        "\n",
        "def pad_sentences(d: dict) -> list:\n",
        "    max_len = get_max_sentence_length(d)\n",
        "    tensor_dict = {}\n",
        "    len_dict = {}\n",
        "    for sen_id, sen_vecs in tqdm(d.items()):\n",
        "        len_dict[sen_id] = len(sen_vecs)\n",
        "        new_vecs = []\n",
        "        for i in range(max_len):\n",
        "            if i < len(sen_vecs):\n",
        "                new_vecs.append(torch.from_numpy(sen_vecs[i]))\n",
        "            else:\n",
        "                new_vecs.append(torch.zeros(200))\n",
        "        tensor_dict[sen_id] = torch.stack(new_vecs)\n",
        "    return tensor_dict, len_dict\n",
        "\n",
        "\n",
        "def plot(x, y, plot_type, save_path):\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "    path = os.path.join(save_path, f\"train_{plot_type}.png\")\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(plot_type)\n",
        "    plt.title(f\"Training {plot_type} per Epoch\")\n",
        "    plt.grid()\n",
        "    plt.savefig(path)\n",
        "    plt.clf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GOt81dTTZYf0"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class SarcasmDataset(Dataset):\n",
        "    def __init__(self, data, labels, lengths):\n",
        "        \"\"\"\n",
        "        PyTorch dataset class\n",
        "        Args:\n",
        "            data - list[list[]]\n",
        "            labels - list()\n",
        "        Return:\n",
        "            None\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.lengths = lengths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "\n",
        "        labels = np.array(self.labels[index])\n",
        "        labels = torch.from_numpy(labels).long()\n",
        "        sen = self.data[index]\n",
        "        l = self.lengths[index]\n",
        "        # print(f\"l.shape: {l.shape}\")\n",
        "\n",
        "        return sen, labels, l\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td1faiVIZYf1",
        "outputId": "8760134e-defa-4830-cc5e-0219b0fb7738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22891/22891 [00:16<00:00, 1351.50it/s]\n",
            "100%|██████████| 5722/5722 [00:00<00:00, 13777.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SarcasmDetectionModel(\n",
            "  (lstm): LSTM(200, 256, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "  (linear): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=36, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=36, out_features=1, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "start training on glove representation\n",
            "Epoch: 1 | Epoch Time: 0m 15s\n",
            "Train Loss: 0.418 | Train Acc: 80.369%\n",
            "Epoch: 2 | Epoch Time: 0m 15s\n",
            "Train Loss: 0.322 | Train Acc: 86.060%\n",
            "Epoch: 3 | Epoch Time: 0m 15s\n",
            "Train Loss: 0.265 | Train Acc: 88.777%\n",
            "Epoch: 4 | Epoch Time: 0m 15s\n",
            "Train Loss: 0.212 | Train Acc: 91.077%\n",
            "Epoch: 5 | Epoch Time: 0m 16s\n",
            "Train Loss: 0.152 | Train Acc: 93.979%\n",
            "Epoch: 6 | Epoch Time: 0m 15s\n",
            "Train Loss: 0.097 | Train Acc: 96.349%\n",
            "Epoch: 7 | Epoch Time: 0m 15s\n",
            "Train Loss: 0.061 | Train Acc: 97.789%\n",
            "Epoch: 8 | Epoch Time: 0m 15s\n",
            "Train Loss: 0.044 | Train Acc: 98.429%\n",
            "Epoch: 9 | Epoch Time: 0m 15s\n",
            "Train Loss: 0.038 | Train Acc: 98.669%\n",
            "Epoch: 10 | Epoch Time: 0m 15s\n",
            "Train Loss: 0.024 | Train Acc: 99.160%\n",
            "finished training on glove representation\n"
          ]
        }
      ],
      "source": [
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        sen, labels, lengths = batch\n",
        "        sen = sen.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # lengths = lengths.to(device)\n",
        "\n",
        "        predictions = model(sen, lengths).squeeze(1)\n",
        "        labels = labels.float()\n",
        "        loss = criterion(predictions, labels)\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "\n",
        "            sen, labels, lengths = batch\n",
        "            sen = sen.to(device)\n",
        "            labels = labels.to(device)\n",
        "            lengths = lengths.float()\n",
        "            sen = sen.float()\n",
        "\n",
        "            predictions = model(sen, lengths).squeeze(1)\n",
        "            labels = labels.float()\n",
        "            loss = criterion(predictions, labels)\n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "DATA_SAVE_PATH = \"\"\n",
        "PLOT_PATH = \"plots/\"\n",
        "\n",
        "# get all the arguments\n",
        "\n",
        "ATTRIBUTE = \"NN\"\n",
        "bz = 64\n",
        "dropout = 0.2\n",
        "num_epochs = 10\n",
        "num_layers = 2\n",
        "hidden_dim = 256\n",
        "lr = 1e-3\n",
        "input_dim = 200\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_sen_vec = load_file(f\"{DATA_SAVE_PATH}train_sen_2_vec.pkl\")\n",
        "train_sen_labels = load_file(f\"{DATA_SAVE_PATH}train_sen_2_label.pkl\")\n",
        "test_sen_vec = load_file(f\"{DATA_SAVE_PATH}test_sen_2_vec.pkl\")\n",
        "test_sen_labels = load_file(f\"{DATA_SAVE_PATH}test_sen_2_label.pkl\")\n",
        "\n",
        "\n",
        "train_lengths = [len(val) for val in train_sen_vec.values()]\n",
        "test_lengths = [len(val) for val in test_sen_vec.values()]\n",
        "\n",
        "train_data, train_length = pad_sentences(train_sen_vec)\n",
        "test_data, test_length = pad_sentences(test_sen_vec)\n",
        "\n",
        "# save_file_pickle(train_data, f\"{DATA_SAVE_PATH}train_sen_tensor.pkl\")\n",
        "# save_file_pickle(train_length, f\"{DATA_SAVE_PATH}train_len_tensor.pkl\")\n",
        "# save_file_pickle(test_data, f\"{DATA_SAVE_PATH}test_sen_tensor.pkl\")\n",
        "# save_file_pickle(test_length, f\"{DATA_SAVE_PATH}data\\\\test_len_tensor.pkl\")\n",
        "\n",
        "train_dataset = SarcasmDataset(list(train_data.values()), list(\n",
        "    train_sen_labels.values()), list(train_length.values()))\n",
        "test_dataset = SarcasmDataset(list(test_data.values()), list(\n",
        "    test_sen_labels.values()), list(test_length.values()))\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=bz, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=bz, shuffle=False)\n",
        "\n",
        "model = SarcasmDetectionModel(input_dim, hidden_dim, num_layers, dropout)\n",
        "print(model)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "\n",
        "print(\"start training on glove representation\")\n",
        "train_losses, train_accs = [], []\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(\n",
        "        model, train_dataloader, optimizer, criterion, device)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
        "    print(f\"Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.3f}%\")\n",
        "print(\"finished training on glove representation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "gQEDB2raZYf2",
        "outputId": "39db6893-9ee3-43d8-e182-846c11a88ff9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot(list(range(num_epochs)), train_losses, \"Loss\",\n",
        "     os.path.join(PLOT_PATH))\n",
        "plot(list(range(num_epochs)), train_accs, \"Accuracy\",\n",
        "     os.path.join(PLOT_PATH))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-ZyP5LQZYf2",
        "outputId": "0877b635-d808-4222-9117-b13768484261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating the model on the glove representation\n",
            "Test Loss: 0.674 | Test Acc: 87.086%\n",
            "testing projected vectors\n",
            "running on NN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5722/5722 [00:00<00:00, 7079.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating the model on the projected representation\n",
            "Test Loss: 0.796 | Test Acc: 84.396%\n",
            "running on JJ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5722/5722 [00:00<00:00, 7700.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating the model on the projected representation\n",
            "Test Loss: 1.180 | Test Acc: 80.413%\n"
          ]
        }
      ],
      "source": [
        "print(\"evaluating the model on the glove representation\")\n",
        "test_loss, test_acc = evaluate(model, test_dataloader, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.3f}%\")\n",
        "# plot(list(range(num_epochs)), train_losses, \"Loss\",\n",
        "#      os.path.join(PLOT_PATH, \"train_loss.png\"))\n",
        "# plot(list(range(num_epochs)), train_accs, \"Accuracy\",\n",
        "#      os.path.join(PLOT_PATH, \"train_accuracy.png\"))\n",
        "\n",
        "print(\"testing projected vectors\")\n",
        "for ATTRIBUTE in [\"NN\", \"JJ\"]:\n",
        "    print(f\"running on {ATTRIBUTE}\")\n",
        "    projected_sen_vec = load_file(\n",
        "        f\"{DATA_SAVE_PATH}test_sen_2_vec_{ATTRIBUTE}.pkl\")\n",
        "    projected_sen_labels = load_file(\n",
        "        f\"{DATA_SAVE_PATH}test_sen_2_label_{ATTRIBUTE}.pkl\")\n",
        "\n",
        "    projected_lengths = [len(val) for val in test_sen_vec.values()]\n",
        "\n",
        "    projected_data, projected_length = pad_sentences(projected_sen_vec)\n",
        "\n",
        "\n",
        "    projected_dataset = SarcasmDataset(list(projected_data.values()), list(\n",
        "        projected_sen_labels.values()), list(projected_length.values()))\n",
        "\n",
        "    projected_dataloader = DataLoader(\n",
        "        projected_dataset, batch_size=bz, shuffle=False)\n",
        "\n",
        "\n",
        "    print(\"evaluating the model on the projected representation\")\n",
        "    test_loss, test_acc = evaluate(\n",
        "        model, projected_dataloader, criterion, device)\n",
        "    print(f\"Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.3f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "final_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}